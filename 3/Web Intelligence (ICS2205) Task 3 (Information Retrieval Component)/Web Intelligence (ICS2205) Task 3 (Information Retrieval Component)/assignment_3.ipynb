{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364ec06b",
   "metadata": {},
   "source": [
    "# Individual Information Retrieval Task for ICS 2205 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36074a0e",
   "metadata": {},
   "source": [
    "#  Document indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868d7c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing nltk library\n",
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d17338d",
   "metadata": {},
   "source": [
    "## 1. Parse the document to extract the data in the XML’s < raw > tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from xml.etree.ElementTree import parse\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Getting a list of all files in the folder\n",
    "folder = \"docs\"\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# Looping over the files in the folder\n",
    "for file in files:\n",
    "    # Parsing the .naf files using nltk's ElementTree parser\n",
    "    file_path = os.path.join(folder, file)\n",
    "    tree = parse(file_path)\n",
    "\n",
    "    # Finding the <raw> tag\n",
    "    raw_tag = tree.find(\"raw\")\n",
    "    # Extracting contents from <raw> tag\n",
    "    raw = raw_tag.text\n",
    "\n",
    "    # Remove all punctuation from the raw text using a regular expression\n",
    "    raw_text = re.sub(r'[{}]'.format(string.punctuation), '', raw)\n",
    "\n",
    "    # Printing the extracted text without punctuation\n",
    "    print(raw_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d99c0b",
   "metadata": {},
   "source": [
    "## 2. Tokenise the documents’ content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    # Parsing the .naf files using nltk's ElementTree parser\n",
    "    file_path = os.path.join(folder, file)\n",
    "    tree = parse(file_path)\n",
    "\n",
    "    # Finding the <raw> tag\n",
    "    raw_tag = tree.find(\"raw\")\n",
    "    # Extracting contents from <raw> tag\n",
    "    raw = raw_tag.text\n",
    "    \n",
    "     # Remove all punctuation from the raw text using a regular expression\n",
    "    raw_text = re.sub(r'[{}]'.format(string.punctuation), '', raw)\n",
    "    \n",
    "    # Tokenizing the text into words or tokens\n",
    "    tokens = word_tokenize(raw_text)\n",
    "\n",
    "    # Printing the tokens\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ccde97",
   "metadata": {},
   "source": [
    "## 3. Perform case-folding, stop-word removal and stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ec7750",
   "metadata": {},
   "source": [
    "### Case Folding:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0192c2",
   "metadata": {},
   "source": [
    "* **Lower Case:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572fe6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    \n",
    "    # Parsing the .naf files using nltk's ElementTree parser\n",
    "    file_path = os.path.join(folder, file)\n",
    "    tree = parse(file_path)\n",
    "\n",
    "    # Finding the <raw> tag\n",
    "    raw_tag = tree.find(\"raw\")\n",
    "    # Extracting contents from <raw> tag\n",
    "    raw = raw_tag.text\n",
    "    \n",
    "    # Remove all punctuation from the raw text using a regular expression\n",
    "    raw_text = re.sub(r'[{}]'.format(string.punctuation), '', raw)\n",
    "    \n",
    "    # Using the .lower() function to convert the string to lowercase\n",
    "    lowercase = raw_text.lower()\n",
    "    # Printing lowercase text\n",
    "    print(lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5436de57",
   "metadata": {},
   "source": [
    "* **Upper Case:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d8e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    \n",
    "    # Parsing the .naf files using nltk's ElementTree parser\n",
    "    file_path = os.path.join(folder, file)\n",
    "    tree = parse(file_path)\n",
    "\n",
    "    # Finding the <raw> tag\n",
    "    raw_tag = tree.find(\"raw\")\n",
    "    # Extracting contents from <raw> tag\n",
    "    raw = raw_tag.text\n",
    "    \n",
    "     # Remove all punctuation from the raw text using a regular expression\n",
    "    raw_text = re.sub(r'[{}]'.format(string.punctuation), '', raw)\n",
    "    \n",
    "    # Using the .upper() function to convert the string to uppercase\n",
    "    uppercase = raw_text.upper()\n",
    "    print(uppercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dfc74f",
   "metadata": {},
   "source": [
    "## Stop-word removal:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6120c5",
   "metadata": {},
   "source": [
    "* In this example, the nltk.corpus.stopwords.words() function is used to get a list of English stop words. The stop words are removed from the list of tokens using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684786a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    \n",
    "    # Parsing the .naf files using nltk's ElementTree parser\n",
    "    file_path = os.path.join(folder, file)\n",
    "    tree = parse(file_path)\n",
    "\n",
    "    # Finding the <raw> tag\n",
    "    raw_tag = tree.find(\"raw\")\n",
    "    # Extracting contents from <raw> tag\n",
    "    raw = raw_tag.text\n",
    "    \n",
    "    # Remove all punctuation from the raw text using a regular expression\n",
    "    raw_text = re.sub(r'[{}]'.format(string.punctuation), '', raw)\n",
    "    \n",
    "    # Getting a list of English stop words\n",
    "    stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "    # Splitting the text into tokens\n",
    "    tokens = nltk.tokenize.word_tokenize(raw_text)\n",
    "\n",
    "    # Removing the stop words from the tokens\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Printing the resulting list of tokens\n",
    "    print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608e7719",
   "metadata": {},
   "source": [
    "## Stemming:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b6f308",
   "metadata": {},
   "source": [
    "* In this example, the Porter stemming algorithm was used\n",
    "\n",
    ">The Porter stemming algorithm (or 'Porter stemmer') is a process for removing the commoner morphological and inflexional endings from words in English. Its main use is as part of a term normalisation process that is usually done when setting up Information Retrieval systems.\n",
    "[Martin, P. (1999). The Porter Stemming Algorithm. [Online]. Available: https://tartarus.org/martin/PorterStemmer ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    # Parsing the .naf files using nltk's ElementTree parser\n",
    "    file_path = os.path.join(folder, file)\n",
    "    tree = parse(file_path)\n",
    "\n",
    "    # Finding the <raw> tag\n",
    "    raw_tag = tree.find(\"raw\")\n",
    "    # Extracting contents from <raw> tag\n",
    "    raw = raw_tag.text\n",
    "    \n",
    "    # Remove all punctuation from the raw text using a regular expression\n",
    "    raw_text = re.sub(r'[{}]'.format(string.punctuation), '', raw)\n",
    "\n",
    "    # Creating a stemmer object\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    # Stemming the raw text\n",
    "    stemmed_text = [stemmer.stem(word) for word in raw_text.split()]\n",
    "    \n",
    "    # Printing stemmed raw text\n",
    "    print(stemmed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b94aae3",
   "metadata": {},
   "source": [
    "## 4. Build the term by document matrix containing the T F.IDF weight for each term within each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52358f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def compute_tfidf(documents):\n",
    "    # Tokenize the documents\n",
    "    tokens = [word_tokenize(doc) for doc in documents]\n",
    "\n",
    "    # Compute the term frequency (TF) for each term\n",
    "    tf = []\n",
    "    for doc in tokens:\n",
    "        term_freq = {}\n",
    "        for term in doc:\n",
    "            if term in term_freq:\n",
    "                term_freq[term] += 1\n",
    "            else:\n",
    "                term_freq[term] = 1\n",
    "        tf.append(term_freq)\n",
    "\n",
    "    # Compute the inverse document frequency (IDF) for each term\n",
    "    idf = {}\n",
    "    num_docs = len(documents)\n",
    "    for doc in tokens:\n",
    "        for term in doc:\n",
    "            if term in idf:\n",
    "                continue\n",
    "            df = sum(1 for d in tokens if term in d)\n",
    "            idf[term] = math.log(num_docs / df)\n",
    "\n",
    "    # Compute the TF-IDF weight for each term\n",
    "    tfidf = []\n",
    "    for doc in tf:\n",
    "        doc_tfidf = {}\n",
    "        for term, freq in doc.items():\n",
    "            doc_tfidf[term] = freq * idf[term]\n",
    "        tfidf.append(doc_tfidf)\n",
    "\n",
    "    # Create the term-by-document matrix\n",
    "    terms = sorted(idf.keys())\n",
    "    term_by_doc_matrix = np.zeros((len(terms), num_docs))\n",
    "    for i, term in enumerate(terms):\n",
    "        for j, doc in enumerate(tfidf):\n",
    "            if term in doc:\n",
    "                term_by_doc_matrix[i, j] = doc[term]\n",
    "\n",
    "    return terms, term_by_doc_matrix\n",
    "\n",
    "# List of documents\n",
    "documents = []\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    # Parsing the .naf files using nltk's ElementTree parser\n",
    "    file_path = os.path.join(folder, file)\n",
    "    tree = parse(file_path)\n",
    "\n",
    "    # Finding the <raw> tag\n",
    "    raw_tag = tree.find(\"raw\")\n",
    "    # Extracting contents from <raw> tag\n",
    "    raw_text = raw_tag.text\n",
    "\n",
    "    # Adding the document to the list of documents\n",
    "    documents.append(raw_text)\n",
    "\n",
    "# Compute the term-by-document matrix\n",
    "terms, term_by_doc_matrix = compute_tfidf(documents)\n",
    "\n",
    "# Print the TF-IDF weights for each term in the matrix\n",
    "for i, row in enumerate(term_by_doc_matrix):\n",
    "    term = terms[i]\n",
    "    print(f'TF-IDF weights for term \"{term}\": {row}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b853cc",
   "metadata": {},
   "source": [
    "#  Querying Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccff0d1",
   "metadata": {},
   "source": [
    "* ### Get a user query – note that it can be set within the notebook directly. Into a variable named query;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088e6788",
   "metadata": {},
   "source": [
    "## Outputting all the quries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65bbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Construct the path to the \"queries\" folder\n",
    "folder = os.path.join(cwd, \"queries\")\n",
    "# Get a list of all files in the \"queries\" folder\n",
    "files = os.listdir(folder)\n",
    "\n",
    "# Loop over the files in the \"queries\" folder\n",
    "for file in files:\n",
    "    # Parse the .naf file using ElementTree\n",
    "    file_path = os.path.join(folder, file)\n",
    "    tree = ET.parse(file_path)\n",
    "    # Find the <raw> tag\n",
    "    raw_tag = tree.find(\"raw\")\n",
    "    # Extract the contents of the <raw> tag\n",
    "    query_raw_text = raw_tag.text\n",
    "    # Print the extracted text\n",
    "    print(query_raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d4e305",
   "metadata": {},
   "source": [
    "## Selecting one of the quries (choose from above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f97fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the user's query\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "# Print the user's query\n",
    "print(\"Your query:\", query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4146a03",
   "metadata": {},
   "source": [
    "* ## Preprocess the user query (tokenisation, case-folding, stop-word removal and stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1645fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "\n",
    "# Converting the query to lowercase\n",
    "query = query.lower()\n",
    "\n",
    "# Tokenizing the query into words or tokens\n",
    "tokens = word_tokenize(query)\n",
    "\n",
    "# Getting a list of English stop words\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "# Removing the stop words from the tokens\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Creating a stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "# Stemming the tokens\n",
    "query_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# Print the preprocessed tokens\n",
    "print(\"Preprocessed query:\", query_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8362ce",
   "metadata": {},
   "source": [
    "* ## Use cosine similarity to calculate the similarity between the query and each document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f8c21",
   "metadata": {},
   "source": [
    "## Calculating cosine similarity between query and documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f37fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Construct the path to the \"docs\" folder\n",
    "folder = os.path.join(cwd, \"docs\")\n",
    "# Get a list of all files in the \"docs\" folder\n",
    "files = os.listdir(folder)\n",
    "\n",
    "def cosine_similarity(query_tokens, doc_tokens):\n",
    "    # Create a Counter object for each list of tokens\n",
    "    query_counter = Counter(query_tokens)\n",
    "    doc_counter = Counter(doc_tokens)\n",
    "\n",
    "    # Create a list of unique words present in either list of tokens\n",
    "    unique_words = set(query_tokens).union(set(doc_tokens))\n",
    "\n",
    "    # Calculate the dot product of the frequency vectors\n",
    "    dot_product = sum(query_counter[word] * doc_counter[word] for word in unique_words)\n",
    "\n",
    "    # Calculate the Euclidean length of the frequency vectors\n",
    "    query_length = sum(query_counter[word]**2 for word in unique_words)\n",
    "    doc_length = sum(doc_counter[word]**2 for word in unique_words)\n",
    "\n",
    "    # If either length is 0, return 0 as the cosine similarity\n",
    "    if query_length == 0 or doc_length == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate the cosine similarity as the dot product of the frequency vectors divided by the product of their Euclidean lengths\n",
    "    cosine_sim = dot_product / (math.sqrt(query_length) * math.sqrt(doc_length))\n",
    "\n",
    "    return cosine_sim\n",
    "\n",
    "# Iterate over all the documents in the folder\n",
    "for file in files:\n",
    "    # Parsing the .naf files using nltk's ElementTree parser\n",
    "    file_path = os.path.join(folder, file)\n",
    "    tree = parse(file_path)\n",
    "\n",
    "    # Finding the <raw> tag\n",
    "    raw_tag = tree.find(\"raw\")\n",
    "    # Extracting contents from <raw> tag\n",
    "    raw_text_docs = raw_tag.text\n",
    "    \n",
    "    # Converting the raw_text_docs to lowercase\n",
    "    raw_text_docs = raw_text_docs.lower()\n",
    "\n",
    "    # Tokenizing the document into words or tokens\n",
    "    tokens_docs = word_tokenize(raw_text_docs)\n",
    "\n",
    "    # Removing stop words from the tokens\n",
    "    filtered_tokens_docs = [token for token in tokens_docs if token not in stop_words]\n",
    "\n",
    "    # Stemming the tokens\n",
    "    stemmed_tokens_docs = [stemmer.stem(token) for token in filtered_tokens_docs]\n",
    "\n",
    "    # Remove duplicates from the list\n",
    "    no_duplicates = []\n",
    "    for token in stemmed_tokens_docs:\n",
    "        if token not in no_duplicates:\n",
    "            no_duplicates.append(token)\n",
    "\n",
    "    # Calculate the cosine similarity between the query and the current document\n",
    "    cosine_sim = cosine_similarity(query_tokens, no_duplicates)\n",
    "    # Print the cosine similarity between the query and the current document\n",
    "    print(\"File: \" f\"{file} - {cosine_sim}\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d47f75",
   "metadata": {},
   "source": [
    "* ## Output the list of documents as a ranked list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60122a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for file in files:\n",
    "    # Parsing the .naf files using nltk's ElementTree parser\n",
    "    file_path = os.path.join(folder, file)\n",
    "    tree = parse(file_path)\n",
    "\n",
    "    # Finding the <raw> tag\n",
    "    raw_tag = tree.find(\"raw\")\n",
    "    # Extracting contents from <raw> tag\n",
    "    raw_text_docs = raw_tag.text\n",
    "    \n",
    "    # Converting the raw_text_docs to lowercase\n",
    "    raw_text_docs = raw_text_docs.lower()\n",
    "\n",
    "    # Tokenizing the document into words or tokens\n",
    "    tokens_docs = word_tokenize(raw_text_docs)\n",
    "\n",
    "    # Removing stop words from the tokens\n",
    "    filtered_tokens_docs = [token for token in tokens_docs if token not in stop_words]\n",
    "\n",
    "    # Stemming the tokens\n",
    "    stemmed_tokens_docs = [stemmer.stem(token) for token in filtered_tokens_docs]\n",
    "\n",
    "    # Remove duplicates from the list\n",
    "    no_duplicates = []\n",
    "    for token in stemmed_tokens_docs:\n",
    "        if token not in no_duplicates:\n",
    "            no_duplicates.append(token)\n",
    "    \n",
    "    # Calculate the cosine similarity between the query and the current document\n",
    "    cosine_sim = cosine_similarity(query_tokens, no_duplicates)\n",
    "    \n",
    "    # Add the file name and cosine similarity to the results list\n",
    "    results.append((file, cosine_sim))\n",
    "\n",
    "# Sort the results list by cosine similarity\n",
    "ranked_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Iterate over the ranked results and print the file name and cosine similarity\n",
    "for result in ranked_results:\n",
    "    file, cosine_sim = result\n",
    "    print(f\"File: {file} - Similarity: {cosine_sim}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "414c148522c3452b8bca3bc99b333facd4b5aaa0d0e2364d6105bb7336ba2f62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
