{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2116716d",
   "metadata": {},
   "source": [
    "# Task 2: Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5edca",
   "metadata": {},
   "source": [
    "## 1. Provide a jupyter notebook that performs the following text analysis tasks on this dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e336b4cd",
   "metadata": {},
   "source": [
    "## a. Process the news headline text:\n",
    "### i. Parse the JSON files, and extract the text from each record (extract the data in “headline” and “short description”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5fa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open the file and read the contents\n",
    "with open('NewsCategoryDataset_2017_2022.json', 'r') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "# Split the contents into a list of individual JSON objects\n",
    "records = contents.split('\\n')\n",
    "\n",
    "# Iterate over each record and extract the data you want\n",
    "for record in records:\n",
    "    # Skip empty records\n",
    "    if not record:\n",
    "        continue\n",
    "    # Load the JSON object\n",
    "    try:\n",
    "        data = json.loads(record)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error while parsing JSON: {e}\")\n",
    "        continue\n",
    "    # Extract the data you want\n",
    "    headline = data.get('headline')\n",
    "    short_description = data.get('short_description')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436af517",
   "metadata": {},
   "source": [
    "### ii. Perform lexical analyses to extract the separate words, and to fold them all to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3bc745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Additional punctuation characters to remove\n",
    "additional_punctuation = '''!()-[]{};:``'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "# Combine the string.punctuation property with the additional punctuation characters\n",
    "punctuation = string.punctuation + additional_punctuation\n",
    "\n",
    "# Open the file and read the contents\n",
    "with open('NewsCategoryDataset_2017_2022.json', 'r') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "# Split the contents into a list of individual JSON objects\n",
    "records = contents.split('\\n')\n",
    "\n",
    "# Iterate over each record and extract the data you want\n",
    "for record in records:\n",
    "    # Skip empty records\n",
    "    if not record:\n",
    "        continue\n",
    "    # Load the JSON object\n",
    "    try:\n",
    "        data = json.loads(record)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error while parsing JSON: {e}\")\n",
    "        continue\n",
    "    # Extract the data you want\n",
    "    headline = data.get('headline')\n",
    "    short_description = data.get('short_description')\n",
    "\n",
    "    # Tokenize the headline and short description strings\n",
    "    headline_tokens = word_tokenize(headline)\n",
    "    short_description_tokens = word_tokenize(short_description)\n",
    "\n",
    "     # Remove punctuation from the lists of tokens\n",
    "    headline_tokens = [token for token in headline_tokens if token not in punctuation]\n",
    "    short_description_tokens = [token for token in short_description_tokens if token not in punctuation]\n",
    "\n",
    "    # Fold the tokens to lower case\n",
    "    headline_tokens_lower = [token.lower() for token in headline_tokens]\n",
    "    short_description_tokens_lower = [token.lower() for token in short_description_tokens]\n",
    "    \n",
    "    print(\"\\nHeadline: \\n\" ,headline_tokens_lower)\n",
    "    print(\"\\nShort_description: \\n\" ,short_description_tokens_lower)\n",
    "    print(\"\\n--------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cfe314",
   "metadata": {},
   "source": [
    "### iii. Use a standard stop-word list for English to filter out the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e1961f",
   "metadata": {},
   "source": [
    "* In this example, the nltk.corpus.stopwords.words() function is used to get a list of English stop words. The stop words are removed from the list of tokens using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Additional punctuation characters to remove\n",
    "additional_punctuation = '''!()-[]{};:``'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "# Get a list of English stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Combine the string.punctuation property with the additional punctuation characters\n",
    "punctuation = string.punctuation + additional_punctuation\n",
    "\n",
    "# Open the file and read the contents\n",
    "with open('NewsCategoryDataset_2017_2022.json', 'r') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "# Split the contents into a list of individual JSON objects\n",
    "records = contents.split('\\n')\n",
    "\n",
    "# Iterate over each record and extract the data you want\n",
    "for record in records:\n",
    "    # Skip empty records\n",
    "    if not record:\n",
    "        continue\n",
    "    # Load the JSON object\n",
    "    try:\n",
    "        data = json.loads(record)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error while parsing JSON: {e}\")\n",
    "        continue\n",
    "    # Extract the data you want\n",
    "    headline = data.get('headline')\n",
    "    short_description = data.get('short_description')\n",
    "\n",
    "    # Tokenize the headline and short description strings\n",
    "    headline_tokens = word_tokenize(headline)\n",
    "    short_description_tokens = word_tokenize(short_description)\n",
    "\n",
    "     # Remove punctuation from the lists of tokens\n",
    "    headline_tokens = [token for token in headline_tokens if token not in punctuation]\n",
    "    short_description_tokens = [token for token in short_description_tokens if token not in punctuation]\n",
    "\n",
    "    # Fold the tokens to lower case\n",
    "    headline_tokens_lower = [token.lower() for token in headline_tokens]\n",
    "    short_description_tokens_lower = [token.lower() for token in short_description_tokens]\n",
    "    \n",
    "    # Remove stop words from the list of headline tokens\n",
    "    headline_tokens_stop_words = [token for token in headline_tokens_lower if token not in stop_words]\n",
    "\n",
    "    # Remove stop words from the list of short description tokens\n",
    "    short_description_tokens_stop_words = [token for token in short_description_tokens_lower if token not in stop_words]\n",
    "    \n",
    "    print(\"\\nHeadline: \\n\" ,headline_tokens_stop_words)\n",
    "    print(\"\\nShort_description: \\n\" ,short_description_tokens_stop_words)\n",
    "    print(\"\\n--------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1459967d",
   "metadata": {},
   "source": [
    "### iv. Use an implementation of Porter’s stemmer to reduce terms to their stems (note that you may find a ready-made implementation provided that you reference its source)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60cd4e1",
   "metadata": {},
   "source": [
    "> The Porter stemming algorithm (or 'Porter stemmer') is a process for removing the commoner morphological and inflexional endings from words in English. Its main use is as part of a term normalisation\n",
    "process that is usually done when setting up Information Retrieval systems. [Martin, P. (1999). The Porter Stemming Algorithm. [Online]. Available: https://tartarus.org/martin/PorterStemmer ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Additional punctuation characters to remove\n",
    "additional_punctuation = '''!()-[]{};:``'\"\\,<>./?@#$%^&*_~'''\n",
    "\n",
    "# Get a list of English stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Combine the string.punctuation property with the additional punctuation characters\n",
    "punctuation = string.punctuation + additional_punctuation\n",
    "\n",
    "# Open the file and read the contents\n",
    "with open('NewsCategoryDataset_2017_2022.json', 'r') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "# Split the contents into a list of individual JSON objects\n",
    "records = contents.split('\\n')\n",
    "\n",
    "# Create an instance of the PorterStemmer class\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Iterate over each record and extract the data you want\n",
    "for record in records:\n",
    "    # Skip empty records\n",
    "    if not record:\n",
    "        continue\n",
    "    # Load the JSON object\n",
    "    try:\n",
    "        data = json.loads(record)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error while parsing JSON: {e}\")\n",
    "        continue\n",
    "        \n",
    "    # Extract the data you want\n",
    "    headline = data.get('headline')\n",
    "    short_description = data.get('short_description')\n",
    "\n",
    "    # Tokenize the headline and short description strings\n",
    "    headline_tokens = word_tokenize(headline)\n",
    "    short_description_tokens = word_tokenize(short_description)\n",
    "\n",
    "     # Remove punctuation from the lists of tokens\n",
    "    headline_tokens = [token for token in headline_tokens if token not in punctuation]\n",
    "    short_description_tokens = [token for token in short_description_tokens if token not in punctuation]\n",
    "\n",
    "    # Fold the tokens to lower case\n",
    "    headline_tokens_lower = [token.lower() for token in headline_tokens]\n",
    "    short_description_tokens_lower = [token.lower() for token in short_description_tokens]\n",
    "    \n",
    "    # Remove stop words from the list of headline tokens\n",
    "    headline_tokens_filtered = [token for token in headline_tokens_lower if token not in stop_words]\n",
    "    # Remove stop words from the list of short description tokens\n",
    "    short_description_tokens_filtered = [token for token in short_description_tokens_lower if token not in stop_words]\n",
    "    \n",
    "    # Stem the filtered headline tokens\n",
    "    headline_tokens_stemmed = [stemmer.stem(token) for token in headline_tokens_filtered]\n",
    "    # Stem the filtered short description tokens\n",
    "    short_description_tokens_stemmed = [stemmer.stem(token) for token in short_description_tokens_filtered]\n",
    "    \n",
    "    print(\"\\nHeadline: \\n\" ,headline_tokens_stemmed)\n",
    "    print(\"\\nShort_description: \\n\" ,short_description_tokens_stemmed)\n",
    "    print(\"\\n--------------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693e0bb7",
   "metadata": {},
   "source": [
    "## b. Calculate term weights using TF.IDF. Each headline record should be considered as a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402992fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def compute_tfidf(documents):\n",
    "    # Tokenize the documents\n",
    "    tokens = [word_tokenize(doc) for doc in documents]\n",
    "    # Compute the term frequency (TF) for each term\n",
    "    tf = []\n",
    "    for doc in tokens:\n",
    "        term_freq = {}\n",
    "        for term in doc:\n",
    "            if term in term_freq:\n",
    "                term_freq[term] += 1\n",
    "            else:\n",
    "                term_freq[term] = 1\n",
    "        tf.append(term_freq)\n",
    "    # Compute the inverse document frequency (IDF) for each term\n",
    "    idf = {}\n",
    "    num_docs = len(documents)\n",
    "    for doc in tokens:\n",
    "        for term in doc:\n",
    "            if term in idf:\n",
    "                continue\n",
    "            df = sum(1 for d in tokens if term in d)\n",
    "            idf[term] = math.log(num_docs / df)\n",
    "    # Compute the TF-IDF weight for each term\n",
    "    tfidf = []\n",
    "    for doc in tf:\n",
    "        doc_tfidf = {}\n",
    "        for term, freq in doc.items():\n",
    "            doc_tfidf[term] = freq * idf[term]\n",
    "        tfidf.append(doc_tfidf)\n",
    "    # Create the term-by-document matrix\n",
    "    terms = sorted(idf.keys())\n",
    "    term_by_doc_matrix = np.zeros((len(terms), num_docs))\n",
    "    for i, term in enumerate(terms):\n",
    "        for j, doc in enumerate(tfidf):\n",
    "            if term in doc:\n",
    "                term_by_doc_matrix[i, j] = doc[term]\n",
    "    return terms, term_by_doc_matrix\n",
    "\n",
    "# Open the file and read the contents\n",
    "with open('NewsCategoryDataset_2017_2022.json', 'r') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "# Split the contents into a list of individual JSON objects\n",
    "records = contents.split('\\n')\n",
    "\n",
    "# Create an instance of the PorterStemmer class\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Create a list to store the headlines\n",
    "headlines = []\n",
    "\n",
    "# Iterate over each record and extract the data you want\n",
    "for record in records:\n",
    "    # Skip empty records\n",
    "    if not record:\n",
    "        continue\n",
    "    # Load the JSON object\n",
    "    try:\n",
    "        data = json.loads(record)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error while parsing JSON: {e}\")\n",
    "        continue\n",
    "    # Extract the data you want\n",
    "    headline = data.get('headline')\n",
    "    short_description = data.get('short_description')\n",
    "\n",
    "    # Tokenize the headline and short description strings\n",
    "    headline_tokens = word_tokenize(headline)\n",
    "    short_description_tokens = word_tokenize(short_description)\n",
    "\n",
    "     # Remove punctuation from the lists of tokens\n",
    "    headline_tokens = [token for token in headline_tokens if token not in punctuation]\n",
    "    short_description_tokens = [token for token in short_description_tokens if token not in punctuation]\n",
    "\n",
    "    # Fold the tokens to lower case\n",
    "    headline_tokens_lower = [token.lower() for token in headline_tokens]\n",
    "    short_description_tokens_lower = [token.lower() for token in short_description_tokens]\n",
    "    \n",
    "    # Remove stop words from the list of headline tokens\n",
    "    headline_tokens_filtered = [token for token in headline_tokens_lower if token not in stop_words]\n",
    "\n",
    "    # Remove stop words from the list of short description tokens\n",
    "    short_description_tokens_filtered = [token for token in short_description_tokens_lower if token not in stop_words]\n",
    "    \n",
    "    # Stem the filtered headline tokens\n",
    "    headline_tokens_stemmed = [stemmer.stem(token) for token in headline_tokens_filtered]\n",
    "\n",
    "    # Stem the filtered short description tokens\n",
    "    short_description_tokens_stemmed = [stemmer.stem(token) for token in short_description_tokens_filtered]\n",
    "    \n",
    "    # Join the stemmed tokens into a single string\n",
    "    headline_stemmed = ' '.join(headline_tokens_stemmed)\n",
    "    # Add the stemmed headline to the list of headlines\n",
    "    headlines.append(headline_stemmed)\n",
    "\n",
    "# Calculate the term weights using the compute_tfidf function\n",
    "terms, term_by_doc_matrix = compute_tfidf(headlines)\n",
    "\n",
    "for i, row in enumerate(term_by_doc_matrix):\n",
    "    term = terms[i]\n",
    "    print(f'TF-IDF weights for term \"{term}\": {row}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75a6f0",
   "metadata": {},
   "source": [
    "## c. Extract the highest-weighted n% of the terms for each headline category (each JSON record has a field called “category”).\n",
    "### i. Calculate the average term weight for all terms over the documents within each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d84bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from statistics import mean\n",
    "\n",
    "# Open the file and read the contents\n",
    "with open('NewsCategoryDataset_2017_2022.json', 'r') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "# Split the contents into a list of individual JSON objects\n",
    "records = contents.split('\\n')\n",
    "    \n",
    "category_data = {}\n",
    "\n",
    "# Create a list to store the headlines\n",
    "categories = []\n",
    "\n",
    "# Iterate over each record and extract the data you want\n",
    "for record in records:\n",
    "    # Skip empty records\n",
    "    if not record:\n",
    "        continue\n",
    "    # Load the JSON object\n",
    "    try:\n",
    "        data = json.loads(record)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error while parsing JSON: {e}\")\n",
    "        continue\n",
    "        \n",
    "    # Extract the data you want\n",
    "    category = data.get('category')\n",
    "\n",
    "    # Tokenize the headline and category strings\n",
    "    category_tokens = word_tokenize(category)\n",
    "\n",
    "    # Remove punctuation from the lists of tokens\n",
    "    category_tokens = [token for token in category_tokens if token not in punctuation]\n",
    "\n",
    "    # Fold the tokens to lower case\n",
    "    category_tokens_lower = [token.lower() for token in category_tokens]\n",
    "    \n",
    "    # Remove stop words from the list of category tokens\n",
    "    category_tokens_filtered = [token for token in category_tokens_lower if token not in stop_words]\n",
    "\n",
    "    # Stem the filtered category tokens\n",
    "    category_tokens_stemmed = [stemmer.stem(token) for token in category_tokens_filtered]\n",
    "    \n",
    "    print(\"\\nCategory: \\n\" ,category_tokens_stemmed)\n",
    "    print(\"\\n--------------------------------------------------------------------------------------------------------------------------\")  \n",
    "    \n",
    "    # Join the stemmed tokens into a single string\n",
    "    category_stemmed = ' '.join(category_tokens_stemmed)\n",
    "    \n",
    "    # Add the stemmed headline to the list of headlines\n",
    "    categories.append(category_stemmed)\n",
    "\n",
    "# Calculate the term weights using the compute_tfidf function\n",
    "terms, term_by_doc_matrix = compute_tfidf(categories)    \n",
    "\n",
    "for i, row in enumerate(term_by_doc_matrix):\n",
    "    term = terms[i]\n",
    "    #print(f'TF-IDF weights for term \"{term}\": {row}')\n",
    "    #print('\\n')\n",
    "     \n",
    "# Initialize the dictionary to store the average term weights\n",
    "average_term_weights = {}\n",
    "\n",
    "# Initialize the defaultdict to store the sum and count of weights for each category\n",
    "average_term_weights = defaultdict(Counter)\n",
    "\n",
    "# Iterate over each row in the term-by-document matrix\n",
    "for i, row in enumerate(term_by_doc_matrix):\n",
    "    term = terms[i]\n",
    "    # Update the sum and count of weights for each category\n",
    "    for j, weight in enumerate(row):\n",
    "        category = categories[j]\n",
    "        average_term_weights[category]['sum'] += weight\n",
    "        average_term_weights[category]['count'] += 1\n",
    "\n",
    "# Compute the average term weights for each category\n",
    "for category, weights in average_term_weights.items():\n",
    "    average_weight = [weights['sum'] / weights['count']]\n",
    "    average_weight = mean(average_weight)\n",
    "    print(f'Average term weights for category \"{category}\": {average_weight}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a19f76",
   "metadata": {},
   "source": [
    "### ii. Get the highest-weighted n% of the terms for each category. This list of terms, and their corresponding weights will subsequently be used to build a category keyword-cloud. This keyword-cloud will show what concepts each category generally mentions. n can be determined arbitrarily so that the keyword-cloud will contain neither too much nor too few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5415d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the category strings\n",
    "category_tokens = [word_tokenize(category) for category in categories]\n",
    "\n",
    "# Remove punctuation from the lists of tokens\n",
    "category_tokens = [[token for token in tokens if token not in punctuation] for tokens in category_tokens]\n",
    "\n",
    "# Fold the tokens to lower case\n",
    "category_tokens_lower = [[token.lower() for token in tokens] for tokens in category_tokens]\n",
    "\n",
    "# Remove stop words from the list of category tokens\n",
    "category_tokens_filtered = [[token for token in tokens if token not in stop_words] for tokens in category_tokens_lower]\n",
    "\n",
    "# Stem the filtered category tokens\n",
    "category_tokens_stemmed = [[stemmer.stem(token) for token in tokens] for tokens in category_tokens_filtered]\n",
    "\n",
    "# Join the stemmed tokens into a single string\n",
    "categories_stemmed = [' '.join(tokens) for tokens in category_tokens_stemmed]\n",
    "\n",
    "# Calculate the term weights using the compute_tfidf function\n",
    "terms, term_by_doc_matrix = compute_tfidf(categories_stemmed)\n",
    "\n",
    "# Initialize a dictionary to store the top terms for each category\n",
    "category_top_terms = {}\n",
    "\n",
    "def get_top_n_percent_terms(terms, weights, n):\n",
    "    # Sort the list of terms in descending order by their weights\n",
    "    sorted_terms = sorted(range(len(terms)), key=lambda i: weights[i], reverse=True)\n",
    "    \n",
    "    # Calculate the number of terms to select as the top n percent\n",
    "    num_terms = round(len(terms) * n / 100)\n",
    "    \n",
    "    # Return the top n percent of terms\n",
    "    return [terms[i] for i in sorted_terms[:num_terms]]\n",
    "\n",
    "# Initialize a dictionary to store the top terms for each category\n",
    "category_top_terms = {}\n",
    "\n",
    "# Iterate over the list of categories\n",
    "for i, category in enumerate(categories):\n",
    "    # Get the top 40% of terms for the category\n",
    "    top_terms = get_top_n_percent_terms(terms, term_by_doc_matrix[:, i], 40)\n",
    "    \n",
    "    # Store the top terms in the dictionary\n",
    "    category_top_terms[category] = top_terms\n",
    "\n",
    "# Print the top terms for each category\n",
    "for category, top_terms in category_top_terms.items():\n",
    "    print(f\"Top terms for category '{category}': {top_terms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f5c5e",
   "metadata": {},
   "source": [
    "### iii. Extract the details of each category (including the category name, the list of articles in it, and list of highest-weighted terms for each category) as JSON. This will be used in the visualisation application described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb07fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open the file and read the contents\n",
    "with open('NewsCategoryDataset_2017_2022.json', 'r') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "# Split the contents into a list of individual JSON objects\n",
    "records = contents.split('\\n')\n",
    "    \n",
    "category_data = {}\n",
    "\n",
    "# Iterate over each record and extract the data you want\n",
    "for record in records:\n",
    "    # Skip empty records\n",
    "    if not record:\n",
    "        continue\n",
    "    # Load the JSON object\n",
    "    try:\n",
    "        data = json.loads(record)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error while parsing JSON: {e}\")\n",
    "        continue\n",
    "        \n",
    "    # Extract the data you want\n",
    "    category = data.get('category')\n",
    "    link = data.get('link')\n",
    "\n",
    "    # Tokenize the category string\n",
    "    category_tokens = word_tokenize(category)\n",
    "\n",
    "    # Remove punctuation from the list of tokens\n",
    "    category_tokens = [token for token in category_tokens if token not in punctuation]\n",
    "\n",
    "    # Fold the tokens to lower case\n",
    "    category_tokens_lower = [token.lower() for token in category_tokens]\n",
    "\n",
    "    # Remove stop words from the list of tokens\n",
    "    category_tokens_filtered = [token for token in category_tokens_lower if token not in stop_words]\n",
    "\n",
    "    # Stem the filtered tokens\n",
    "    category_tokens_stemmed = [stemmer.stem(token) for token in category_tokens_filtered]\n",
    "\n",
    "    # Join the stemmed tokens into a single string\n",
    "    category_stemmed = ' '.join(category_tokens_stemmed)\n",
    "\n",
    "    # Get the top 40% of terms for the category\n",
    "    top_terms = get_top_n_percent_terms(terms, term_by_doc_matrix[:, i], 40)\n",
    "\n",
    "    # Create a dictionary to store the category details\n",
    "    category_details = {\n",
    "        'category': category,\n",
    "        'links': [link],\n",
    "        'top_terms': top_terms\n",
    "    }\n",
    "\n",
    "    # Check if the category is already in the category data dictionary\n",
    "    if category_stemmed in category_data:\n",
    "        # Update the links and top terms for the category\n",
    "        category_data[category_stemmed]['links'].append(link)\n",
    "        category_data[category_stemmed]['top_terms'] = top_terms\n",
    "    else:\n",
    "        # Add the category details to the category data dictionary\n",
    "        category_data[category_stemmed] = category_details\n",
    "\n",
    "# Open the file to write the JSON data\n",
    "with open('category_data.json', 'w') as f:\n",
    "    # Write the JSON data to the file with indentation\n",
    "    json.dump(category_data, f, indent=2)\n",
    "    \n",
    "print('Output in category_data.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11bea47",
   "metadata": {},
   "source": [
    "## d. Use the document vectors to cluster the news headlines using the k-means algorithm. The choice of k is up to you. Note that you only need to do a single level of clustering, that is, no hierarchies are being requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c64fefb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cluster_headlines(term_by_doc_matrix, num_clusters):\n",
    "    # Transpose the term-by-document matrix to create a document-by-term matrix\n",
    "    doc_by_term_matrix = term_by_doc_matrix.T\n",
    "    # Convert the document-by-term matrix to a dense array\n",
    "    doc_by_term_array = doc_by_term_matrix\n",
    "    # Initialize the k-means model\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    # Fit the model to the document vectors\n",
    "    kmeans.fit(doc_by_term_array)\n",
    "    # Predict the cluster labels for each document\n",
    "    labels = kmeans.predict(doc_by_term_array)\n",
    "    return labels\n",
    "\n",
    "#Open the file and read the contents\n",
    "with open('NewsCategoryDataset_2017_2022.json', 'r') as f:\n",
    "    contents = f.read()\n",
    "\n",
    "# Split the contents into a list of individual JSON objects\n",
    "records = contents.split('\\n')\n",
    "    \n",
    "headline_data = {}\n",
    "\n",
    "# Create a list to store the headlines\n",
    "headlines = []\n",
    "\n",
    "# Iterate over each record and extract the data you want\n",
    "for record in records:\n",
    "    # Skip empty records\n",
    "    if not record:\n",
    "        continue\n",
    "    # Load the JSON object\n",
    "    try:\n",
    "        data = json.loads(record)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error while parsing JSON: {e}\")\n",
    "        continue\n",
    "        \n",
    "    # Extract the data you want\n",
    "    headline = data.get('headline')\n",
    "\n",
    "    # Tokenize the headline and category strings\n",
    "    headline_tokens = word_tokenize(headline)\n",
    "\n",
    "    # Remove punctuation from the lists of tokens\n",
    "    headline_tokens = [token for token in headline_tokens if token not in punctuation]\n",
    "\n",
    "    # Fold the tokens to lower case\n",
    "    headline_tokens_lower = [token.lower() for token in headline_tokens]\n",
    "    \n",
    "    # Remove stop words from the list of category tokens\n",
    "    headline_tokens_filtered = [token for token in headline_tokens_lower if token not in stop_words]\n",
    "\n",
    "    # Stem the filtered category tokens\n",
    "    headline_tokens_stemmed = [stemmer.stem(token) for token in headline_tokens_filtered]\n",
    "    \n",
    "    print(\"headline: \\n\" ,headline_tokens_stemmed)\n",
    "    print(\"\\n--------------------------------------------------------------------------------------------------------------------------\")  \n",
    "    \n",
    "    # Join the stemmed tokens into a single string\n",
    "    headline_stemmed = ' '.join(headline_tokens_stemmed)\n",
    "    \n",
    "    # Add the stemmed headline to the list of headlines\n",
    "    headlines.append(headline_stemmed)\n",
    "    \n",
    "# Calculate the term weights using the compute_tfidf function\n",
    "terms, term_by_doc_matrix = compute_tfidf(headlines)\n",
    "\n",
    "for i, row in enumerate(term_by_doc_matrix):\n",
    "    term = terms[i]\n",
    "    print(f'TF-IDF weights for term \"{term}\": {row}')\n",
    "    print('\\n')\n",
    "\n",
    "# Cluster the headlines\n",
    "num_clusters = 5\n",
    "labels = cluster_headlines(term_by_doc_matrix, num_clusters=num_clusters)\n",
    "\n",
    "# Group the headlines by cluster\n",
    "clustered_headlines = {}\n",
    "for i, label in enumerate(labels):\n",
    "    if label not in clustered_headlines:\n",
    "        clustered_headlines[label] = []\n",
    "    clustered_headlines[label].append(headlines[i])\n",
    "\n",
    "#Print the headlines in each cluster\n",
    "for label, cluster in clustered_headlines.items():\n",
    "    print(f'Cluster {label}:')\n",
    "    for headline in cluster:\n",
    "        print(headline)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac787a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0a18fff",
   "metadata": {},
   "source": [
    "## e. Extract the highest-weighted n% of the terms for each cluster generated in the previous step.\n",
    "### i. Calculate the average term weight for all terms over the documents within each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c9d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the term-by-document matrix to create a document-by-term matrix\n",
    "doc_by_term_matrix = term_by_doc_matrix.T\n",
    "\n",
    "# Create a list to store the average term weights for each cluster\n",
    "avg_term_weights = []\n",
    "\n",
    "# Iterate over each cluster label\n",
    "for label in np.unique(labels):\n",
    "  # Select the documents in the document-by-term matrix that belong to the cluster\n",
    "  cluster_docs = doc_by_term_matrix[labels == label]\n",
    "  # Calculate the sum of the term weights for each term over the selected documents\n",
    "  term_weights_sum = cluster_docs.sum(axis=0)\n",
    "  # Calculate the average term weight for each term in the cluster\n",
    "  avg_term_weight = term_weights_sum / cluster_docs.shape[0]\n",
    "  # Add the average term weights for each term in the cluster to the list\n",
    "  avg_term_weights.append(avg_term_weight)\n",
    "\n",
    "# Print the average term weights for each cluster\n",
    "for cluster_idx, avg_term_weight in enumerate(avg_term_weights):\n",
    "    print(f'Cluster {cluster_idx}:')\n",
    "    for term_idx, avg_weight in enumerate(avg_term_weight):\n",
    "        term = terms[term_idx]\n",
    "        print(f'  Term \"{term}\": {avg_weight: }')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a5f873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b41caf31",
   "metadata": {},
   "source": [
    "### ii. Get the highest-weighted n% of the terms for each cluster. This list of terms, and their corresponding weights will subsequently be used to build a cluster keyword-cloud. This keyword-cloud will show what concepts each cluster generally mentions. n can be determined arbitrarily so that the keyword-cloud will contain neither too much nor too few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da30232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_top_n_percent_terms(labels, terms, weights, n):\n",
    "    # Initialize a dictionary to store the top terms and weights for each cluster\n",
    "    top_terms = defaultdict(list)\n",
    "    \n",
    "    # Iterate over the list of labels\n",
    "    for label in set(labels):\n",
    "        # Extract the term weights for the cluster\n",
    "        cluster_weights = weights[:, labels == label]\n",
    "        # Calculate the average term weights for the cluster\n",
    "        avg_weights = cluster_weights.mean(axis=1)\n",
    "        # Sort the list of terms in descending order by their average weights\n",
    "        sorted_terms = sorted(range(len(terms)), key=lambda i: avg_weights[i], reverse=True)\n",
    "    \n",
    "        # Calculate the number of terms to select as the top n percent\n",
    "        num_terms = round(len(terms) * n / 100)\n",
    "    \n",
    "        # Extract the top n percent of terms for the cluster\n",
    "        cluster_top_terms = [terms[i] for i in sorted_terms[:num_terms]]\n",
    "        \n",
    "        # Extract the corresponding weights for the top terms\n",
    "        cluster_top_weights = [avg_weights[i] for i in sorted_terms[:num_terms]]\n",
    "        \n",
    "        # Store the top terms and weights in the dictionary\n",
    "        top_terms[label] = list(zip(cluster_top_terms, cluster_top_weights))\n",
    "    \n",
    "    return top_terms\n",
    "\n",
    "# Get the top 40% of terms for each cluster\n",
    "n = 40\n",
    "cluster_top_terms = get_top_n_percent_terms(labels, terms, term_by_doc_matrix, n)\n",
    "\n",
    "# Print the top terms and weights for each cluster\n",
    "for label, top_terms in cluster_top_terms.items():\n",
    "    print(f'Cluster {label}:')\n",
    "    for term, weight in top_terms:\n",
    "        print(f'  {term}: {weight: }')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6ecb46",
   "metadata": {},
   "source": [
    "### iii. Extract the details of each cluster (the cluster ID, the list of articles in it, and list of highest-weighted terms for each cluster) as JSON. This will be used in the visualisation application described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e7ea9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Get the cluster labels and the documents in each cluster\n",
    "cluster_labels = labels\n",
    "cluster_docs = term_by_doc_matrix\n",
    "\n",
    "# Load the JSON file containing the links for each document\n",
    "try:\n",
    "    with open('NewsCategoryDataset_2017_2022.json', 'r') as f:\n",
    "        doc_links = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    #print(\"The file 'NewsCategoryDataset_2017_2022.json' was not found.\")\n",
    "    doc_links = []\n",
    "except json.JSONDecodeError:\n",
    "    #print(\"There was an error decoding the JSON file.\")\n",
    "    doc_links = []\n",
    "\n",
    "# Get the top 40% of terms for each cluster\n",
    "n = 40\n",
    "cluster_top_terms = get_top_n_percent_terms(cluster_labels, terms, cluster_docs, n)\n",
    "\n",
    "# Initialize an empty dictionary to store the cluster details\n",
    "cluster_data = {}\n",
    "\n",
    "# Iterate over the cluster labels\n",
    "for label in set(cluster_labels):\n",
    "  # Initialize an empty list to store the links for the cluster\n",
    "  cluster_links = []\n",
    "  # Iterate over the links for each document\n",
    "  for doc_idx, doc_link in enumerate(doc_links):\n",
    "    # Check if the document belongs to the cluster\n",
    "    if cluster_labels[doc_idx] == label:\n",
    "      # If it does, add the link to the list\n",
    "      cluster_links.append(doc_link['link'])\n",
    "  # Extract the top terms and weights for the cluster\n",
    "  cluster_top_term_weights = cluster_top_terms[label]\n",
    "  # Create a dictionary to store the cluster details\n",
    "  cluster_details = {\n",
    "    'cluster_id': str(label),\n",
    "    'links': cluster_links,\n",
    "    'top_terms': cluster_top_term_weights\n",
    "  }\n",
    "  # Add the cluster details to the cluster data dictionary\n",
    "  cluster_data[str(label)] = cluster_details\n",
    "\n",
    "# Open the file to write the JSON data\n",
    "try:\n",
    "    with open('cluster_data.json', 'w') as f:\n",
    "      # Write the JSON data to the file with indentation\n",
    "        json.dump(cluster_data, f, indent=2)\n",
    "    print('Output written to cluster_data.json')\n",
    "except IOError:\n",
    "\n",
    "    print(\"An error occurred while writing to the file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797243d0",
   "metadata": {},
   "source": [
    "#### --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# <u>Instructions on how to start the Flask Application:<u>\n",
    "\n",
    "### 1. Open the \"*web_application*\" folder in a terminal or command prompt and navigate to the directory where the file \"*app.py*\" is saved.\n",
    "### 2. Run the following command (or run from the IDE), to start the Flask development server: python app.py. \n",
    "### 3. After the server starts, you can view the web pages by navigating to http://localhost:5000/ in your web browser.\n",
    "### 4. After navigating to the port, a web page with two clickable links will be shown.\n",
    "### 5. First link will show the Category data. The dropdown menu can be used to choose to view information about a specific category.There is aswell the bubble chart which when a bubble is clicked will show inofrmation about a specific category.\n",
    "### 6. On the other hand, the second link will show the Cluster data. The dropdown menu can be used to view information about a specific cluster.\n",
    "    \n",
    "### <u>Other Notes:<u>\n",
    "* #### The category_data.json and cluster_data.json files must be in the same directory as the script for the code to work correctly.\n",
    "* #### The HTML files are found in the templates folder.\n",
    "* #### The D3.js library was used to implement the Bubble Charts.\n",
    "    \n",
    "### <u>Difficulties:<u>\n",
    "#### 1. The bubble chart for the list of categories (question 2,c) was implemented and the categories' information is outputted when the bubbles are clicked on, but for some reason the bubbles are too zoomed in and only two bubbles can be seen. Eventhough the other bubbles are there, I could not get the correct parameters to show them all.\n",
    "#### 2. The code for the bubble chart for the clusters (question 2,d) could not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8430286d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba18286d27dd3f1705df4025c31d2b7021c67fcd01f023a6f194ed0db21ec453"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
